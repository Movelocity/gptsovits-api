'''
æŒ‰ä¸­è‹±æ··åˆè¯†åˆ«
æŒ‰æ—¥è‹±æ··åˆè¯†åˆ«
å¤šè¯­ç§å¯åŠ¨åˆ‡åˆ†è¯†åˆ«è¯­ç§
å…¨éƒ¨æŒ‰ä¸­æ–‡è¯†åˆ«
å…¨éƒ¨æŒ‰è‹±æ–‡è¯†åˆ«
å…¨éƒ¨æŒ‰æ—¥æ–‡è¯†åˆ«
'''
import os, re, logging
import LangSegment
if False:
    logging.getLogger("markdown_it").setLevel(logging.ERROR)
    logging.getLogger("urllib3").setLevel(logging.ERROR)
    logging.getLogger("httpcore").setLevel(logging.ERROR)
    logging.getLogger("httpx").setLevel(logging.ERROR)
    logging.getLogger("asyncio").setLevel(logging.ERROR)
    logging.getLogger("charset_normalizer").setLevel(logging.ERROR)
    logging.getLogger("torchaudio._extension").setLevel(logging.ERROR)
# import pdb

import gradio as gr
from transformers import AutoModelForMaskedLM, AutoTokenizer
import numpy as np
import librosa, torch
import soundfile as sf

from module.models import SynthesizerTrn
from AR.models.t2s_lightning_module import Text2SemanticLightningModule
from text import cleaned_text_to_sequence
from text.cleaner import clean_text
from time import time as ttime
from module.mel_processing import spectrogram_torch
from my_utils import load_audio
from i18n.i18n import I18nAuto
from cuts import splits, cuts, merge_short_text_in_array


import whisper
###################### global variables #######################
i18n = I18nAuto()

gpt_path = "pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt"  # gweight.txt
sovits_path = "pretrained_models/s2G488k.pth"  # sweight.txt
bert_path = "pretrained_models/chinese-roberta-wwm-ext-large"

infer_tts_port = int(os.environ.get("infer_tts_port", 9872))

is_share = eval(os.environ.get("is_share", "False"))
is_half = eval(os.environ.get("is_half", "True"))
print(f"is_half: {is_half}")

dtype = torch.float16 if is_half else torch.float32

if "_CUDA_VISIBLE_DEVICES" in os.environ:
    os.environ["CUDA_VISIBLE_DEVICES"] = os.environ["_CUDA_VISIBLE_DEVICES"]

os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'  # ç¡®ä¿ç›´æ¥å¯åŠ¨æ¨ç†UIæ—¶ä¹Ÿèƒ½å¤Ÿè®¾ç½®ã€‚

device = "cuda" if torch.cuda.is_available() else 'cpu'

from feature_extractor import cnhubert
cnhubert_base_path = "pretrained_models/chinese-hubert-base"
# cnhubert.cnhubert_base_path = cnhubert_base_path

dict_language = {
    i18n("ä¸­æ–‡"): "all_zh",     # å…¨éƒ¨æŒ‰ä¸­æ–‡è¯†åˆ«
    i18n("è‹±æ–‡"): "en",         # å…¨éƒ¨æŒ‰è‹±æ–‡è¯†åˆ«#######ä¸å˜
    i18n("æ—¥æ–‡"): "all_ja",     # å…¨éƒ¨æŒ‰æ—¥æ–‡è¯†åˆ«
    i18n("ä¸­è‹±æ··åˆ"): "zh",     # æŒ‰ä¸­è‹±æ··åˆè¯†åˆ«####ä¸å˜
    i18n("æ—¥è‹±æ··åˆ"): "ja",     # æŒ‰æ—¥è‹±æ··åˆè¯†åˆ«####ä¸å˜
    i18n("å¤šè¯­ç§æ··åˆ"): "auto", # å¤šè¯­ç§å¯åŠ¨åˆ‡åˆ†è¯†åˆ«è¯­ç§
}

# langs = [
#     "all_zh",     # å…¨éƒ¨æŒ‰ä¸­æ–‡è¯†åˆ«
#     "en",         # å…¨éƒ¨æŒ‰è‹±æ–‡è¯†åˆ«#######ä¸å˜
#     "all_ja",     # å…¨éƒ¨æŒ‰æ—¥æ–‡è¯†åˆ«
#     "zh",     # æŒ‰ä¸­è‹±æ··åˆè¯†åˆ«####ä¸å˜
#     "ja",     # æŒ‰æ—¥è‹±æ··åˆè¯†åˆ«####ä¸å˜
#     "auto", # å¤šè¯­ç§å¯åŠ¨åˆ‡åˆ†è¯†åˆ«è¯­ç§
# ]


model_dict = {
    "ssl_model": None,
    "tokenizer": None,
    "bert_model": None,
    "whisper_model": None,
    "vq_model": None,
    "hps": None,
    "hz": 50,
    "max_sec": None,
}

def load_ssl_model():
    if model_dict['ssl_model'] is None:
        print("[æ¨¡å‹åŠ è½½] CNHubert")
        t1 = ttime()
        ssl_model = cnhubert.CNHubert(cnhubert_base_path).eval()
        if is_half:
            ssl_model = ssl_model.half()
        model_dict['ssl_model'] = ssl_model.to(device)
        t2 = ttime()
        print(f'time used: {t2-t1}')

load_ssl_model()

def get_tokenizer():
    if model_dict['tokenizer'] is None:
        model_dict['tokenizer'] = AutoTokenizer.from_pretrained(bert_path)
    return model_dict['tokenizer']

def get_bert_model():
    if model_dict['bert_model'] is None:
        print('loading bert model...')
        t1 = ttime()
        bert_model = AutoModelForMaskedLM.from_pretrained(bert_path).eval()
        if is_half:
            bert_model = bert_model.half()
        model_dict['bert_model'] = bert_model.to(device)
        t2 = ttime()
        print(f'time used: {t2-t1}')
    return model_dict['bert_model']

def get_whisper_model():
    if model_dict['whisper_model'] is None:
        print('loading whisper model...')
        t1 = ttime()
        # ä¸‹è½½æ¨¡å‹, options: ['tiny', 'base', 'small', 'medium', 'large']
        model_dict['whisper_model'] = whisper.load_model("small")  
        t2 = ttime()
        print(f'time used: {t2-t1}')
    return model_dict['whisper_model']

###################################################################

def speech2text(audio_path):
    if not audio_path:
        return ''
    print(f"reading: {audio_path}")

    whisper_model = get_whisper_model()

    t1 = ttime()
    result = whisper_model.transcribe(audio_path, language='zh', initial_prompt='ä»¥ä¸‹æ˜¯æ™®é€šè¯ï¼š')
    t2 = ttime()
    print(f'asr time: {t2 - t1}')
    return result['text']


class DictToAttrRecursive(dict):
    def __init__(self, input_dict):
        super().__init__(input_dict)
        for key, value in input_dict.items():
            if isinstance(value, dict):
                value = DictToAttrRecursive(value)
            self[key] = value
            setattr(self, key, value)

    def __getattr__(self, item):
        try:
            return self[item]
        except KeyError:
            raise AttributeError(f"Attribute {item} not found")

    def __setattr__(self, key, value):
        if isinstance(value, dict):
            value = DictToAttrRecursive(value)
        super(DictToAttrRecursive, self).__setitem__(key, value)
        super().__setattr__(key, value)

    def __delattr__(self, item):
        try:
            del self[item]
        except KeyError:
            raise AttributeError(f"Attribute {item} not found")


def change_sovits_weights(sovits_path):
    dict_s2 = torch.load(sovits_path, map_location="cpu")
    hps = dict_s2["config"]

    hps = DictToAttrRecursive(hps)
    hps.model.semantic_frame_rate = "25hz"
    print("Loading VITS2 module...")
    t1 = ttime()
    vq_model = SynthesizerTrn(
        hps.data.filter_length // 2 + 1,
        hps.train.segment_size // hps.data.hop_length,
        n_speakers=hps.data.n_speakers,
        **hps.model
    ).eval()
    if ("pretrained" not in sovits_path):
        del vq_model.enc_q
    vq_model.load_state_dict(dict_s2["weight"], strict=False)

    if is_half == True:
        vq_model = vq_model.half()
    model_dict['vq_model'] = vq_model.to(device)

    t2 = ttime()
    print(f"time used: {t2 - t1}")

    model_dict['hps'] = hps


def change_gpt_weights(gpt_path):
    print("Loading GPT module...")
    dict_s1 = torch.load(gpt_path, map_location="cpu")
    config = dict_s1["config"]
    model_dict['max_sec'] = config["data"]["max_sec"]

    t1 = ttime()
    t2s_model = Text2SemanticLightningModule(config, "****", is_train=False).eval()
    t2s_model.load_state_dict(dict_s1["weight"])
    if is_half == True:
        t2s_model = t2s_model.half()
    t2s_model = t2s_model.to(device)
    t2 = ttime()
    print(f"time used: {t2 - t1}")

    total = sum([param.nelement() for param in t2s_model.parameters()])
    print("Number of parameter: %.2fM" % (total / 1e6))

    model_dict['t2s_model'] = t2s_model

change_sovits_weights(sovits_path)
change_gpt_weights(gpt_path)


zero_wav = np.zeros(
    int(model_dict['hps'].data.sampling_rate * 0.3),  # 0.3 ç§’
    dtype=np.float16 if is_half == True else np.float32,
)

def get_prompt(wav_path):
    with torch.no_grad():
        wav16k, sr = librosa.load(wav_path, sr=16000)
        if (wav16k.shape[0] > 160000 or wav16k.shape[0] < 48000):
            raise ValueError(i18n("å‚è€ƒéŸ³é¢‘åœ¨3~10ç§’èŒƒå›´å¤–ï¼Œè¯·æ›´æ¢ï¼"))
        
        wav16k = torch.from_numpy(wav16k)
        zero_wav_torch = torch.from_numpy(zero_wav)
        if is_half == True:
            wav16k = wav16k.half()
            zero_wav_torch = zero_wav_torch.half()
        wav16k = wav16k.to(device)
        zero_wav_torch = zero_wav_torch.to(device)

        wav16k = torch.cat([wav16k, zero_wav_torch])
        ssl_content = model_dict['ssl_model'].model(wav16k.unsqueeze(0))["last_hidden_state"].transpose(1, 2)  # .float()
        # print(f"> ssl_content.shape: {ssl_content.shape}") # batch, dim, seq_len # [1, 768, 280]

        codes = model_dict['vq_model'].extract_latent(ssl_content)
        prompt_semantic = codes[0, 0].unsqueeze(0).to(device)
        # print(f"> voice prompt shape: {prompt_semantic.shape}")  # [1, 140]
    return prompt_semantic


def get_spec(cfg, filename):
    audio = load_audio(filename, int(cfg.sampling_rate))
    audio_norm = torch.FloatTensor(audio)
    audio_norm = audio_norm.unsqueeze(0)
    spec = spectrogram_torch(
        audio_norm,
        cfg.filter_length,
        cfg.sampling_rate,
        cfg.hop_length,
        cfg.win_length,
        center=False,
    )
    return spec

############ bert processing ##################

def get_bert_feature(text, word2ph):
    assert len(word2ph) == len(text)

    tokenizer = get_tokenizer()
    bert_model = get_bert_model()  # chinese-roberta-wwm-ext-large

    with torch.no_grad():
        inputs = tokenizer(text, return_tensors="pt")
        for i in inputs:
            inputs[i] = inputs[i].to(device)
        res = bert_model(**inputs, output_hidden_states=True)
        res = torch.cat(res["hidden_states"][-3:-2], -1)[0].cpu()[1:-1]
    
    """
    >>> a
    tensor([1, 2, 3])
    >>> a.repeat(3)
    tensor([1, 2, 3, 1, 2, 3, 1, 2, 3])
    >>> a.repeat(3, 1)
    tensor([[1, 2, 3],
            [1, 2, 3],
            [1, 2, 3]])
    """
    phone_level_feature = []
    for i, repeats in enumerate(word2ph):
        repeat_feature = res[i].repeat(repeats, 1)
        phone_level_feature.append(repeat_feature)
    phone_level_feature = torch.cat(phone_level_feature, dim=0)
    # print(f"bert_feature shape: {phone_level_feature.shape}")
    return phone_level_feature.T

def clean_text_inf(text, language):
    phones, word2ph, norm_text = clean_text(text, language)
    phones = cleaned_text_to_sequence(phones)
    return phones, word2ph, norm_text

def get_bert_inf(phones, word2ph, norm_text, language):
    language=language.replace("all_","")
    if language == "zh":
        bert = get_bert_feature(norm_text, word2ph).to(device)#.to(dtype)
    else:
        bert = torch.zeros(
            (1024, len(phones)),
            dtype=torch.float16 if is_half == True else torch.float32,
        ).to(device)
    return bert

def get_phones_and_bert(text, language):
    if language in {"en", "all_zh", "all_ja"}:
        language = language.replace("all_","")

        if language == "en":
            LangSegment.setfilters(["en"])
            formattext = " ".join(tmp["text"] for tmp in LangSegment.getTexts(text))
        else:
            # æ— æ³•åŒºåˆ«ä¸­æ—¥æ–‡æ±‰å­—,ä»¥ç”¨æˆ·è¾“å…¥ä¸ºå‡†
            formattext = text

        while "  " in formattext:
            formattext = formattext.replace("  ", " ")

        phones, word2ph, norm_text = clean_text_inf(formattext, language)

        if language == "zh":
            bert = get_bert_feature(norm_text, word2ph).to(device)
        else:
            bert = torch.zeros(
                (1024, len(phones)),
                dtype=torch.float16 if is_half == True else torch.float32,
            ).to(device)
    elif language in {"zh", "ja", "auto"}:
        LangSegment.setfilters(["zh", "ja", "en"])
        textlist, langlist = [], []
        if language == "auto":
            for tmp in LangSegment.getTexts(text):
                langlist.append(tmp["lang"])
                textlist.append(tmp["text"])
        else:
            for tmp in LangSegment.getTexts(text):
                if tmp["lang"] == "en":
                    langlist.append(tmp["lang"])
                else:
                    # å› æ— æ³•åŒºåˆ«ä¸­æ—¥æ–‡æ±‰å­—,ä»¥ç”¨æˆ·è¾“å…¥ä¸ºå‡†
                    langlist.append(language)
                textlist.append(tmp["text"])

        print(f"{textlist} {langlist}")

        phones_list, bert_list, norm_text_list = [], [], []
        for i in range(len(textlist)):
            lang = langlist[i]
            phones, word2ph, norm_text = clean_text_inf(textlist[i], lang)
            bert = get_bert_inf(phones, word2ph, norm_text, lang)
            phones_list.append(phones)
            norm_text_list.append(norm_text)
            bert_list.append(bert)
        bert = torch.cat(bert_list, dim=1).to(dtype)
        phones = sum(phones_list, [])
        norm_text = ''.join(norm_text_list)

    return phones, bert, norm_text

############ text splitter ###################


cuts_map = {
    i18n("å‡‘å››å¥ä¸€åˆ‡"): "every_four_sentences",
    i18n("å‡‘50å­—ä¸€åˆ‡"): "every_50_chars",
    i18n("æŒ‰ä¸­æ–‡å¥å·ã€‚åˆ‡"): "ã€‚",
    i18n("æŒ‰è‹±æ–‡å¥å·.åˆ‡"): ".",
    i18n("æŒ‰æ ‡ç‚¹ç¬¦å·åˆ‡"): "sign",
    i18n("ä¸åˆ‡"): "none"
}


def split_text(text, how_to_cut=i18n("ä¸åˆ‡")):
    pattern = "[" + "".join(re.escape(sep) for sep in splits) + "]"
    first_word = re.split(pattern, text.strip())[0].strip()
    
    if text[0] not in splits and len(first_word) < 4: 
        text = ("ã€‚" if text_language != "en" else ".") + text

    # print("target text: ", text)
    cut_name = cuts_map.get(how_to_cut)
    cut_fn = cuts.get(cut_name)
    text = cut_fn(text)

    while "\n\n" in text:
        text = text.replace("\n\n", "\n")
    texts = text.split("\n")
    texts = merge_short_text_in_array(texts, threshold=5)
    print(f"segments: {texts}")
    return texts

##################### main tts ###########################

def get_tts_wav(ref_wav_path, prompt_text, prompt_language, text, text_language, how_to_cut=i18n("ä¸åˆ‡"), top_k=20, top_p=0.6, temperature=0.6, ref_free = False):
    if prompt_text is None or len(prompt_text) == 0:
        ref_free = True

    prompt_language = dict_language[prompt_language]
    text_language = dict_language[text_language]
    if not ref_free:
        prompt_text = prompt_text.strip("\n")
        if (prompt_text[-1] not in splits): prompt_text += "ã€‚" if prompt_language != "en" else "."
        # print("prompt_text: ", prompt_text)

        prompt_phonemes, bert1, norm_text1 = get_phones_and_bert(prompt_text, prompt_language)

    texts = split_text(text, how_to_cut)

    voice_prompt = get_prompt(ref_wav_path)
    refer_spec = get_spec(model_dict['hps'].data, ref_wav_path)  # .to(device)
    if is_half == True:
        refer_spec = refer_spec.half()
    refer_spec = refer_spec.to(device)

    audio_opt = []
    for text in texts:
        # è§£å†³è¾“å…¥ç›®æ ‡æ–‡æœ¬çš„ç©ºè¡Œå¯¼è‡´æŠ¥é”™çš„é—®é¢˜
        if (len(text.strip()) == 0):
            continue
        if (text[-1] not in splits): 
            text += "ã€‚" if text_language != "en" else "."
        # print(i18n("å®é™…è¾“å…¥çš„ç›®æ ‡æ–‡æœ¬(æ¯å¥):"), text)

        phonemes, bert2, norm_text2 = get_phones_and_bert(text, text_language)
        # print(i18n("å‰ç«¯å¤„ç†åçš„æ–‡æœ¬(æ¯å¥):"), norm_text2)
        if not ref_free:
            bert_feat = torch.cat([bert1, bert2], 1)
            all_phoneme_ids = torch.LongTensor(prompt_phonemes + phonemes).to(device).unsqueeze(0)
        else:
            bert_feat = bert2
            all_phoneme_ids = torch.LongTensor(phonemes).to(device).unsqueeze(0)

        bert_feat = bert_feat.to(device).unsqueeze(0)
        all_phoneme_len = torch.tensor([all_phoneme_ids.shape[-1]]).to(device)

        with torch.no_grad():
            pred_semantic, idx = model_dict['t2s_model'].model.infer_panel(
                all_phoneme_ids,
                all_phoneme_len,
                None if ref_free else voice_prompt,
                bert_feat,
                # prompt_phone_len=ph_offset,
                top_k=top_k,
                top_p=top_p,
                temperature=temperature,
                early_stop_num=model_dict['hz'] * model_dict['max_sec'],
            )
        # print(pred_semantic.shape,idx)
        pred_semantic = pred_semantic[:, -idx:].unsqueeze(0)  # .unsqueeze(0)  # mq è¦å¤š unsqueeze ä¸€æ¬¡
        
        # audio = vq_model.decode(pred_semantic, all_phoneme_ids, refer).detach().cpu().numpy()[0, 0]
        audio = (
            model_dict['vq_model'].decode(
                pred_semantic, 
                torch.LongTensor(phonemes).to(device).unsqueeze(0), 
                refer_spec
            ).detach().cpu().numpy()[0, 0]
        )  ### è¯•è¯•é‡å»ºä¸å¸¦ä¸Š prompt çš„éƒ¨åˆ†
        
        max_audio=np.abs(audio).max()  # ç®€å•é˜²æ­¢16bitçˆ†éŸ³
        if max_audio > 1: audio /= max_audio
        audio_opt.append(audio)
        audio_opt.append(zero_wav)

    # é‡‡æ ·ç‡, 16bit éŸ³é¢‘
    # return hps.data.sampling_rate, (np.concatenate(audio_opt, 0) * 32768).astype(np.int16)
    audio_16bit = (np.concatenate(audio_opt, 0) * 32768).astype(np.int16)
    sf.write('TEMP/tmp.mp3', audio_16bit, model_dict['hps'].data.sampling_rate)
    return os.path.join(os.getcwd(), 'TEMP', 'tmp.mp3')





def custom_sort_key(s):
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å­—ç¬¦ä¸²ä¸­çš„æ•°å­—éƒ¨åˆ†å’Œéæ•°å­—éƒ¨åˆ†
    parts = re.split('(\d+)', s)
    # å°†æ•°å­—éƒ¨åˆ†è½¬æ¢ä¸ºæ•´æ•°ï¼Œéæ•°å­—éƒ¨åˆ†ä¿æŒä¸å˜
    parts = [int(part) if part.isdigit() else part for part in parts]
    return parts


def change_choices():
    SoVITS_names, GPT_names = get_weights_names()
    return {
        "choices": sorted(SoVITS_names, key=custom_sort_key),
        "__type__": "update"
    }, {
        "choices": sorted(GPT_names, key=custom_sort_key), 
        "__type__": "update"
    }


def get_weights_names():
    pretrained_sovits_name = "pretrained_models/s2G488k.pth"
    pretrained_gpt_name = "pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt"
    SoVITS_weight_root = "pretrained_models/SoVITS_weights"
    GPT_weight_root = "pretrained_models/GPT_weights"
    os.makedirs(SoVITS_weight_root, exist_ok=True)
    os.makedirs(GPT_weight_root, exist_ok=True)

    SoVITS_names = [pretrained_sovits_name]
    for name in os.listdir(SoVITS_weight_root):
        if name.endswith(".pth"): 
            SoVITS_names.append("%s/%s" % (SoVITS_weight_root, name))
    GPT_names = [pretrained_gpt_name]
    for name in os.listdir(GPT_weight_root):
        if name.endswith(".ckpt"): 
            GPT_names.append("%s/%s" % (GPT_weight_root, name))
    return SoVITS_names, GPT_names

# SoVITSæ¨¡å‹åˆ—è¡¨, GPTæ¨¡å‹åˆ—è¡¨
SoVITS_names, GPT_names = get_weights_names()

refresh_symbol = '\U0001f504'  # ğŸ”„

class FormComponent:
    def get_expected_parent(self):
        return gr.components.Form
class FormRow(FormComponent, gr.Row):
    """Same as gr.Row but fits inside gradio forms"""
    def get_block_name(self):
        return "row"

# theme spacing_size='md', radius_size='md'

from cuts import cut1, cut2, cut3, cut4, cut5

test_text = """æˆ‘å¯¹Japanese cultureæœ‰ç€ç‹¬ç‰¹çš„çƒ­çˆ±ï¼Œå°¤å…¶æ˜¯songå’Œanimation. æˆ‘æ˜¯åŸç¥çš„å¿ å®ç²‰ä¸ï¼Œå¦‚æœä½ ä¹Ÿæ˜¯ï¼Œæˆ‘ä»¬ä¸€å®šå¯ä»¥æˆä¸ºå¥½æœ‹å‹ã€‚åŸç¥ï¼å¯åŠ¨ï¼
In a summary, æˆ‘æ˜¯ä¸€ä½çƒ­çˆ±ç”Ÿæ´»ã€å·¥ä½œå’Œå­¦ä¹ çš„äººã€‚æœŸå¾…åœ¨è¿™é‡Œä¸å¤§å®¶å…±åŒè¿›æ­¥ï¼Œå…±åˆ›è¾‰ç…Œï¼"""

with gr.Blocks(title="GPT-SoVITS WebUI", theme=gr.themes.Default()) as app:
    with gr.Tab(label=i18n("*å‚è€ƒè¯­éŸ³")):
        with gr.Row():
            with gr.Column():
                inp_ref = gr.Audio(label=i18n("è¯·ä¸Šä¼ 3~10ç§’å†…å‚è€ƒéŸ³é¢‘ï¼Œè¶…è¿‡ä¼šæŠ¥é”™ï¼"), type="filepath")
                with gr.Row():
                    prompt_language = gr.Dropdown(
                        label="*"+i18n("å‚è€ƒéŸ³é¢‘çš„è¯­ç§"),
                        choices=[i18n("ä¸­æ–‡"), i18n("è‹±æ–‡"), i18n("æ—¥æ–‡"), i18n("ä¸­è‹±æ··åˆ"), i18n("æ—¥è‹±æ··åˆ"), i18n("å¤šè¯­ç§æ··åˆ")], 
                        value=i18n("ä¸­æ–‡")
                    )
                    with gr.Column():
                        ref_text_free = gr.Checkbox(
                            label=i18n("å¼€å¯æ— å‚è€ƒæ–‡æœ¬æ¨¡å¼ã€‚ä¸å¡«å‚è€ƒæ–‡æœ¬äº¦ç›¸å½“äºå¼€å¯ã€‚"), 
                            info=i18n("ä½¿ç”¨æ— å‚è€ƒæ–‡æœ¬æ¨¡å¼æ—¶å»ºè®®ä½¿ç”¨å¾®è°ƒçš„GPTï¼Œå¬ä¸æ¸…å‚è€ƒéŸ³é¢‘è¯´çš„å•¥(ä¸æ™“å¾—å†™å•¥)å¯ä»¥å¼€ï¼Œå¼€å¯åæ— è§†å¡«å†™çš„å‚è€ƒæ–‡æœ¬ã€‚"),
                            value=False, interactive=True, show_label=True)
                
            with gr.Column():
                prompt_text = gr.Textbox(label=i18n("å‚è€ƒéŸ³é¢‘çš„æ–‡æœ¬"), value="", lines=2)
                asr_btn = gr.Button(value='auto sound recognition')
                asr_btn.click(speech2text, inputs=[inp_ref], outputs=[prompt_text])
                # inp_ref.change(fn=asr, inputs=inp_ref, outputs=prompt_text)
                
        with gr.Tab("TTS"):
            with gr.Row():
                with gr.Column():
                    with gr.Row():
                        text_language = gr.Dropdown(
                            label=i18n("éœ€è¦åˆæˆçš„è¯­ç§"), 
                            choices=[i18n("ä¸­æ–‡"), i18n("è‹±æ–‡"), i18n("æ—¥æ–‡"), i18n("ä¸­è‹±æ··åˆ"), i18n("æ—¥è‹±æ··åˆ"), i18n("å¤šè¯­ç§æ··åˆ")], 
                            value=i18n("ä¸­è‹±æ··åˆ")
                        )
                        how_to_cut = gr.Dropdown(
                            label=i18n("æ€ä¹ˆåˆ‡"),
                            choices=[i18n("ä¸åˆ‡"), i18n("å‡‘å››å¥ä¸€åˆ‡"), i18n("å‡‘50å­—ä¸€åˆ‡"), i18n("æŒ‰ä¸­æ–‡å¥å·ã€‚åˆ‡"), i18n("æŒ‰è‹±æ–‡å¥å·.åˆ‡"), i18n("æŒ‰æ ‡ç‚¹ç¬¦å·åˆ‡"), ],
                            value=i18n("æŒ‰ä¸­æ–‡å¥å·ã€‚åˆ‡"),
                        )
                    gr.Markdown("gpté‡‡æ ·å‚æ•°(æ— å‚è€ƒæ–‡æœ¬æ—¶ä¸è¦å¤ªä½)ï¼š")
                    top_k = gr.Slider(minimum=1,maximum=100,step=1,label=i18n("top_k"),value=5,interactive=True)
                    top_p = gr.Slider(minimum=0,maximum=1,step=0.05,label=i18n("top_p"),value=0.95,interactive=True)
                    temperature = gr.Slider(minimum=0,maximum=1,step=0.05,label=i18n("temperature"),value=0.95,interactive=True)
                        
                with gr.Column():
                    text = gr.Textbox(label=i18n("éœ€è¦åˆæˆçš„æ–‡æœ¬"), value="", lines=3, placeholder="Text for audio synthesis")
                    inference_button = gr.Button(i18n("åˆæˆè¯­éŸ³"), variant="primary")
                    output = gr.Audio(label=i18n("è¾“å‡ºçš„è¯­éŸ³"))
                    with gr.Accordion(label='examples', open=False):
                        gr.Examples([test_text], text)
        inference_button.click(
            get_tts_wav,
            [inp_ref, prompt_text, prompt_language, text, text_language, how_to_cut, top_k, top_p, temperature, ref_text_free],
            [output],
        )

    with gr.Tab(label=i18n("æ¨¡å‹åˆ‡æ¢")):
        with gr.Row():
            GPT_dropdown = gr.Dropdown(label=i18n("GPTæ¨¡å‹åˆ—è¡¨"), choices=sorted(GPT_names, key=custom_sort_key), value=gpt_path, interactive=True)
            SoVITS_dropdown = gr.Dropdown(label=i18n("SoVITSæ¨¡å‹åˆ—è¡¨"), choices=sorted(SoVITS_names, key=custom_sort_key), value=sovits_path, interactive=True)
            refresh_button = gr.Button(value=refresh_symbol)
            refresh_button.click(fn=change_choices, inputs=[], outputs=[SoVITS_dropdown, GPT_dropdown])

            SoVITS_dropdown.change(change_sovits_weights, [SoVITS_dropdown], [])
            GPT_dropdown.change(change_gpt_weights, [GPT_dropdown], [])


    with gr.Accordion(label="Tool", open=False):
        gr.Markdown(value=i18n("æ–‡æœ¬åˆ‡åˆ†å·¥å…·ã€‚å¤ªé•¿çš„æ–‡æœ¬åˆæˆå‡ºæ¥æ•ˆæœä¸ä¸€å®šå¥½ï¼Œæ‰€ä»¥å¤ªé•¿å»ºè®®å…ˆåˆ‡ã€‚åˆæˆä¼šæ ¹æ®æ–‡æœ¬çš„æ¢è¡Œåˆ†å¼€åˆæˆå†æ‹¼èµ·æ¥ã€‚"))
        with gr.Row():
            with gr.Column():
                text_inp = gr.Textbox(label=i18n("éœ€è¦åˆæˆçš„åˆ‡åˆ†å‰æ–‡æœ¬"), value="")
                with gr.Row():
                    button1 = gr.Button(i18n("å‡‘å››å¥ä¸€åˆ‡"), variant="secondary")
                    button2 = gr.Button(i18n("å‡‘50å­—ä¸€åˆ‡"), variant="secondary")
                    button3 = gr.Button(i18n("æŒ‰ä¸­æ–‡å¥å·ã€‚åˆ‡"), variant="secondary")
                    button4 = gr.Button(i18n("æŒ‰è‹±æ–‡å¥å·.åˆ‡"), variant="secondary")
                    button5 = gr.Button(i18n("æŒ‰æ ‡ç‚¹ç¬¦å·åˆ‡"), variant="secondary")
            text_opt = gr.Textbox(label=i18n("åˆ‡åˆ†åæ–‡æœ¬"), value="", lines=4)
            button1.click(cut1, [text_inp], [text_opt])
            button2.click(cut2, [text_inp], [text_opt])
            button3.click(cut3, [text_inp], [text_opt])
            button4.click(cut4, [text_inp], [text_opt])
            button5.click(cut5, [text_inp], [text_opt])
        gr.Markdown(value=i18n("åç»­å°†æ”¯æŒè½¬éŸ³ç´ ã€æ‰‹å·¥ä¿®æ”¹éŸ³ç´ ã€è¯­éŸ³åˆæˆåˆ†æ­¥æ‰§è¡Œã€‚"))

# queue è¡¨ç¤ºå¯ä»¥ç»™å¤šä¸ªuserä½¿ç”¨ï¼Œè®©ä»–ä»¬æ’é˜Ÿ
app.queue(concurrency_count=511, max_size=1022).launch(
    server_name="0.0.0.0",
    inbrowser=True,  # æ‰“å¼€æµè§ˆå™¨
    share=is_share,
    server_port=infer_tts_port,
    quiet=True,
)
